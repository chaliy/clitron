# Supervised Fine-Tuning Configuration
# This config is optimized for Apple Silicon (MPS) with 16-32GB unified memory

model:
  # Base model to fine-tune
  name: "meta-llama/Llama-3.2-1B-Instruct"

training:
  output_dir: "./models/sft"
  num_train_epochs: 3
  per_device_train_batch_size: 4  # Reduced for MPS memory constraints
  gradient_accumulation_steps: 8  # Increased to maintain effective batch size
  learning_rate: 2.0e-5
  warmup_ratio: 0.1
  lr_scheduler_type: "cosine"
  weight_decay: 0.01
  max_grad_norm: 1.0

  # LoRA configuration for efficient training
  use_lora: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  lora_target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"

data:
  train_file: "./data/train.jsonl"
  validation_file: "./data/train_val.jsonl"
  max_seq_length: 2048

logging:
  report_to: "none"  # Set to "wandb" for W&B logging
  logging_steps: 10
